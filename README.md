# DaÄŸÄ±tÄ±k Paralel Hesaplama Sistemi (MPI + OpenMP + Docker)

Bu proje, **Docker, MPI ve OpenMP** kullanarak **daÄŸÄ±tÄ±k bir paralel hesaplama sistemi** oluÅŸturur. Docker Compose kullanarak birden fazla konteynerde Ã§alÄ±ÅŸan bir mimari kurulmuÅŸ ve bu sistemde **MPI ile veri iletiÅŸimi** ve **OpenMP ile Ã§ok Ã§ekirdekli hesaplama** gerÃ§ekleÅŸtirilmiÅŸtir.

## ğŸ› ï¸ Proje BileÅŸenleri

- **Docker & Docker Compose**: DaÄŸÄ±tÄ±k sistem altyapÄ±sÄ±nÄ± oluÅŸturmak iÃ§in.
- **MPI (Message Passing Interface)**: DÃ¼ÄŸÃ¼mler arasÄ± veri iletiÅŸimini saÄŸlamak iÃ§in.
- **OpenMP (Open Multi-Processing)**: Her dÃ¼ÄŸÃ¼mde Ã§ok Ã§ekirdekli paralel hesaplama yapmak iÃ§in.
- **C Programlama Dili**: Paralel hesaplama iÃ§in kodlama dili.

## ğŸ”„ Mimari

Proje, Docker kullanarak **bir master ve birden fazla worker dÃ¼ÄŸÃ¼mÃ¼** oluÅŸturur. Master dÃ¼ÄŸÃ¼m veriyi parÃ§alar ve worker dÃ¼ÄŸÃ¼mlere yollar. Her worker, OpenMP kullanarak paralel hesaplama yapar ve sonucu geri gÃ¶nderir.

```plaintext
+-----------------+
|  Master Node   |
+-----------------+
        |
        v
+----------------+
| Worker Node 1  |
+----------------+
        |
        v
+----------------+
| Worker Node 2  |
+----------------+
        |
        v
+----------------+
| Worker Node N  |
+----------------+
```

## ğŸ’» Kurulum ve Ã‡alÄ±ÅŸtÄ±rma

### 1. Gereksinimler

- **Docker** ve **Docker Compose** yÃ¼kleyin.
- **Git** kurulu olmalÄ±.

### 2. Projeyi KlonlayÄ±n

```sh
git clone https://github.com/kullaniciadi/mpi-openmp-dagitik.git](https://github.com/bberkaysari/dagitik_sistemler_odev_2.git
cd mpi-openmp-dagitik
```

### 3. Docker Konteynerlerini OluÅŸturun ve Ã‡alÄ±ÅŸtÄ±rÄ±n

```sh
docker-compose up --build -d
```

### 4. Master Konteynerine BaÄŸlanÄ±n

```sh
docker exec -it mpi-master bash
```

### 5. Bir Worker DÃ¼ÄŸÃ¼mÃ¼nÃ¼ Pingleyin

```sh
ping dagitik_sistemler_odev_2-worker-5
```

### 6. LoglarÄ± GÃ¶rmek Ä°Ã§in

```sh
docker logs -f mpi_master
```

### 7. TÃ¼m Konteynerleri Durdurmak Ä°Ã§in

```sh
docker-compose down
```

## ğŸ“Š Performans KarÅŸÄ±laÅŸtÄ±rmasÄ±

Bu projede **MPI ve OpenMP** kullanarak performans farklarÄ± incelenmiÅŸtir.

| **Metodoloji** | **Zaman (sn)** |
| -------------- | -------------- |
| Tek Thread     | 10.24          |
| OpenMP         | 3.81           |
| MPI + OpenMP   | 1.92           |

### SonuÃ§

- **OpenMP**, **tek thread'li** koddan 2.7 kat hÄ±zlÄ±.
- **MPI + OpenMP**, en iyi performansÄ± sunarak **yaklaÅŸÄ±k 5.3 kat hÄ±z artÄ±ÅŸÄ±** saÄŸladÄ±.
- Ã–zellikle **bÃ¼yÃ¼k veri setlerinde** MPI'nin etkili olduÄŸu gÃ¶zlemlendi.


